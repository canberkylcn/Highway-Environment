# Destination 'o1' olduğu için bunun Intersection ortamı olduğunu varsayıyoruz
env_id: "intersection-v1"
render_mode: null

env_params:
  observation:
    type: Kinematics
    vehicles_count: 15  # Etraftaki 15 aracı görsün (Kör nokta kalmasın)
    features:
      - presence
      - x
      - y
      - vx
      - vy
      - cos_h
      - sin_h
    features_range:
      x: [-100, 100]
      y: [-100, 100]
      vx: [-20, 20]
      vy: [-20, 20]
    absolute: true
    
    # [KRİTİK DÜZELTME 1]: PPO/DQN (MlpPolicy) için bu MUTLAKA true olmalı.
    flatten: true
    
    # [KRİTİK DÜZELTME 2]: Ajanın diğer araçların niyetini (dönüş sinyali) okumasını sağlar.
    observe_intentions: true

  # 2. AKSİYON (ACTION): Ajanın "Elleri ve Ayakları"
  action:
    type: DiscreteMetaAction
    longitudinal: true
    
    # [HAYATİ DÜZELTME]: Kavşakta dönebilmesi için true olmak zorunda.
    lateral: true
    
    # [0]: Tam durma (Sabır), [4.5]: Yavaş kontrol, [9]: Hızlı geçiş
    target_speeds: [0, 4.5, 9]

  # 3. SİMÜLASYON DİNAMİKLERİ
  duration: 25  # [HAYATİ]: Sabır için süre uzatıldı (13sn -> 25sn)
  destination: o1
  controlled_vehicles: 1
  initial_vehicle_count: 10
  spawn_probability: 0.6  # Yoğunluk seviyesi
  screen_width: 600
  screen_height: 600
  centering_position: [0.5, 0.6]
  scaling: 7.15  # Python'daki (5.5 * 1.3) işleminin sonucu
  
  # 4. ÖDÜL SİSTEMİ (REWARD SHAPING)
  collision_reward: -5  # Kaza cezası
  arrived_reward: 10    # Başarı ödülü (Risk almaya değmesi için artırıldı)
  high_speed_reward: 1
  reward_speed_range: [0.0, 9.0] # Dururken ceza yememesi için 0'dan başlatıldı
  normalize_reward: true
  offroad_terminal: false

# Ajan ayarlarını senin önceki başarılı DQN stratejine göre ekledim
agent_params:
  algorithm: "DQN"
  total_timesteps: 300000
  save_path: "models/intersection_custom_dqn"
  tensorboard_log: "logs/tensorboard/"
  checkpoint_freq: 150000
  
  model_params:
    learning_rate: 0.0005
    buffer_size: 50000
    learning_starts: 1000
    batch_size: 32
    tau: 1.0
    gamma: 0.99
    train_freq: 4
    gradient_steps: 1
    target_update_interval: 1000
    exploration_fraction: 0.3
    exploration_initial_eps: 1.0
    exploration_final_eps: 0.05
    verbose: 1
    # device: "auto"  <--- BU SATIRI SİL (DELETE THIS LINE)
    
    policy_kwargs:
      net_arch: [256, 256]